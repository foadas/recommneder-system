{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-09T19:58:37.994214Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8061ef1ce8683ae7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-09T19:58:37.995214Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read the CSV file and limit to the first 5000 rows\n",
    "items = pd.read_csv('train_users.csv')\n",
    "# Count the number of occurrences for each category in the 'title_fa_category' column\n",
    "category_counts = items['title_fa_category'].value_counts()\n",
    "# Display the counts for each category\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb786739bb45e41",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-08-09T19:58:37.997217Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType, StructType, StructField, StringType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"DistributedIncrementalCosineSimilarity\").getOrCreate()\n",
    "\n",
    "# Define schema for the sample data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"rate\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Hardcoded sample data\n",
    "data = [\n",
    "    (\"1\", \"Item1\", 4.0),\n",
    "    (\"2\", \"Item2\", 5.0),\n",
    "    (\"3\", \"Item3\", 3.0),\n",
    "    (\"4\", \"Item4\", 2.0),\n",
    "    (\"5\", \"Item5\", 4.5),\n",
    "    (\"6\", \"Item6\", 3.5),\n",
    "    (\"7\", \"Item7\", 4.0),\n",
    "    (\"8\", \"Item8\", 5.0),\n",
    "    (\"9\", \"Item9\", 2.5),\n",
    "    (\"10\", \"Item10\", 3.5)\n",
    "]\n",
    "\n",
    "# Create DataFrame from sample data\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Initial DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Convert 'rate' column to FloatType (already correct in this case)\n",
    "df = df.withColumn(\"rate\", col(\"rate\").cast(FloatType()))\n",
    "\n",
    "# Vectorize the rate column\n",
    "assembler = VectorAssembler(inputCols=[\"rate\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Display DataFrame after vectorization\n",
    "print(\"DataFrame after VectorAssembler:\")\n",
    "df.select(\"rate\", \"features\").show()\n",
    "\n",
    "# Normalize the ratings\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Fit the scaler and transform the data\n",
    "try:\n",
    "    scaler_model = scaler.fit(df)\n",
    "    df = scaler_model.transform(df)\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred during StandardScaler fit: {e}\")\n",
    "\n",
    "# Display DataFrame after scaling\n",
    "print(\"DataFrame after StandardScaler:\")\n",
    "df.select(\"scaled_features\").show()\n",
    "\n",
    "# Function to calculate cosine similarity incrementally\n",
    "def cosine_similarity_incremental(chunk_df, broadcast_chunk):\n",
    "    similarities = []\n",
    "    chunk_array = np.array(chunk_df.select(\"scaled_features\").rdd.map(lambda x: x[0].toArray()).collect())\n",
    "    broadcast_array = np.array(broadcast_chunk.value.select(\"scaled_features\").rdd.map(lambda x: x[0].toArray()).collect())\n",
    "    \n",
    "    for i, vec1 in enumerate(chunk_array):\n",
    "        for j, vec2 in enumerate(broadcast_array):\n",
    "            if i != j:  # Skip self-similarity\n",
    "                sim = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "                similarities.append((chunk_df.collect()[i]['id'], broadcast_chunk.value.collect()[j]['id'], sim))\n",
    "    return similarities\n",
    "\n",
    "# Split data into chunks and compute similarities\n",
    "num_chunks = 2\n",
    "total_rows = df.count()\n",
    "chunk_size = total_rows // num_chunks\n",
    "\n",
    "# Initialize a list to store all similarities\n",
    "all_similarities = []\n",
    "\n",
    "# Create chunks\n",
    "for i in range(num_chunks):\n",
    "    # Create a chunk DataFrame\n",
    "    lower_bound = i * chunk_size\n",
    "    upper_bound = (i + 1) * chunk_size\n",
    "    chunk_df = df.limit(upper_bound).subtract(df.limit(lower_bound))\n",
    "    \n",
    "    # Broadcast the chunk for similarity computation\n",
    "    broadcast_chunk = spark.sparkContext.broadcast(chunk_df.collect())\n",
    "    \n",
    "    # Compute similarities for the current chunk\n",
    "    similarities = cosine_similarity_incremental(chunk_df, broadcast_chunk)\n",
    "    all_similarities.extend(similarities)\n",
    "\n",
    "# Print out a few examples of the computed similarities\n",
    "print(\"Computed Similarities:\")\n",
    "for sim in all_similarities[:10]:  # Display first 10 similarities as an example\n",
    "    print(f\"Item {sim[0]} and Item {sim[1]} have similarity {sim[2]:.4f}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389cb381a96b7f46",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:51.303085Z",
     "start_time": "2024-08-09T21:57:48.926085Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "comments_df = pd.read_csv('digikala/2-comments.csv')\n",
    "products_df = pd.read_csv('digikala/5-products.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "راستش رو بخواین وقتی رفتم کارتریجش زرو شارژ کنم یه فردی که ۳۰ سال کارش پرینتر و از این جور چیزا بود بهم گفت پرینتر ماهی خریدی...\n",
      "حالا دیگه تصمیم با شما برای کارهای اداری عالیه و به درد اونایی که زیاد پرینت میگیرن بیشتر میخوره حرف نداره برای کارای سنگین محشره بهترین پرینتری که تا به حال استفاده کردم\n",
      "کم مصرف و بصرفه . لوازم مصرفی با پایینترین هزینه تعویض و سرویس میشه\n",
      "در یک کلام بهترین برای پرینت تعداد بالا\n",
      "اما قیمت الان بالاس\n",
      " 4 سال پیش اکبند خریدم 480 تومن با کارتریج اصلی . سه تا کارتریج دیگه هم همراهش خریدم و تا الان فقط با همین کارتریجها استفاده کردم . فقط شارژ کردم و لوازم مصرفی عوض کردم و سرویس و مجدد پرینت زدم.\n",
      "دفتر فنی و امور اینترنتی دارم و تعداد پرینتم بالاس . البته در کنارش یک دستگاه کپی شارپ هم دارم ولی میانگین در روز بیش از 200 برگ پرینت رو باهاش میزنم . فوق العادس\n",
      "تنها ایرادش نداشتن wifi هست بسیار عالی\n",
      "پرینتری بینظیر\n"
     ]
    }
   ],
   "source": [
    "#removing unnecessary data\n",
    "#products_df = products_df.drop(columns=['product_attributes'])\n",
    "\n",
    "new_comments_df = comments_df.groupby('product_id', as_index=False).agg({\n",
    "    'title': lambda x: ' '.join(str(i) for i in x.unique()),  # Convert to string and concatenate unique titles\n",
    "    'comment': lambda x: ' '.join(str(i) for i in x),          # Convert to string and concatenate all comments\n",
    "    'advantages': lambda x: ' '.join(str(i) for i in x),       # Convert to string and concatenate all advantages\n",
    "    'disadvantages': lambda x: ' '.join(str(i) for i in x)    \n",
    "})\n",
    "print(new_comments_df['comment'][0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.090082Z",
     "start_time": "2024-08-09T21:57:51.306085Z"
    }
   },
   "id": "c146436d863350c3",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_comments_df.to_csv('new_comment.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.694083Z",
     "start_time": "2024-08-09T21:57:57.091083Z"
    }
   },
   "id": "efd53a085a7626f2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Merge the DataFrames on 'product_id' from comments_df and 'id' from products_df\n",
    "merged_df = pd.merge( products_df, new_comments_df,left_on='id', right_on='product_id')\n",
    "\n",
    "# Optional: Save the resulting DataFrame to a new CSV file\n",
    "merged_df.to_csv('merged_output.csv', index=False)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.826085Z",
     "start_time": "2024-08-09T21:57:57.695082Z"
    }
   },
   "id": "1f62eb69-f722-4fc0-8366-656253eef1f0",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique product IDs: 5708\n"
     ]
    }
   ],
   "source": [
    "merged_df = pd.read_csv('merged_output.csv')\n",
    "\n",
    "# Count the number of unique product IDs\n",
    "unique_product_ids = merged_df['product_id'].nunique()\n",
    "\n",
    "print(f\"Number of unique product IDs: {unique_product_ids}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.939086Z",
     "start_time": "2024-08-09T21:57:57.828084Z"
    }
   },
   "id": "e24393fce0bd029a",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "merged_df = merged_df.fillna('')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.952086Z",
     "start_time": "2024-08-09T21:57:57.940084Z"
    }
   },
   "id": "b8b516b5655d8372",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0       Savoy Burn Relief Spray 50ml کرم و ژل ترمیم کن...\n",
      "1        ساعت مچی عقربه‌ای ساعت،ساعت زنانه،ساعت ارزان،...\n",
      "2        کیف پول کیف ، کارت ، کارت ویزیت ، کارت بانکی ...\n",
      "3        رو بالشی روبالشی ، کالای خواب ، روتختی ، کودک...\n",
      "4        مایو مایو مردانه مایو ورزشیشلوارک ورزشیشلوارک...\n",
      "                              ...                        \n",
      "5703     اسپری اسپری،خوشبو کننده،بدن،مردانه،ضد تعریق،د...\n",
      "5704    Unique 1727 Flask 06 Liter کلمن و فلاسک فلاسک ...\n",
      "5705     آبکش و آبگیر  آبکش و آبگیر VirgoBasin متفرقه ...\n",
      "5706     سایر لوازم تزئینی بادبزن بادبزن دستی باد بزن ...\n",
      "5707     استند و پایه خنک کننده پایه خنک کننده سادیتا ...\n",
      "Name: combined_features, Length: 5708, dtype: object\n",
      "(5708, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import string\n",
    "\n",
    "merged_df['combined_features'] = merged_df['product_title_en'] + ' ' + merged_df['category_title_fa']+ ' ' + merged_df['title_alt']+ ' ' + merged_df['category_keywords']+ ' ' + merged_df['brand_name_fa'] + ' ' + merged_df['brand_name_en']\n",
    "#print(merged_df['combined_features'])\n",
    "nan_product_id = merged_df['product_title_en'].isna().sum()\n",
    "nan_combined_features = merged_df['combined_features'].isna().sum()\n",
    "# Remove all punctuation\n",
    "merged_df['combined_features'] = merged_df['combined_features'].str.replace(f'[{string.punctuation}]', '', regex=True)\n",
    "\n",
    "print(nan_product_id, nan_combined_features)\n",
    "print(merged_df['combined_features'])\n",
    "print(merged_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.991091Z",
     "start_time": "2024-08-09T21:57:57.954087Z"
    }
   },
   "id": "a0ceb01ed902ca4e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:57.996084Z",
     "start_time": "2024-08-09T21:57:57.992085Z"
    }
   },
   "id": "fef4a265a50e0cde",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Split the text into separate words\n",
    "#merged_df['combined_features'] = merged_df['combined_features'].str.split()\n",
    "\n",
    "#print(merged_df['combined_features'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:58.002086Z",
     "start_time": "2024-08-09T21:57:57.997086Z"
    }
   },
   "id": "f1e062f01f0778b",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5708, 14315)\n",
      "جولری\n",
      "product_id    723227    725786  725253    727088    724768    724514  \\\n",
      "product_id                                                             \n",
      "723227      1.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
      "725786      0.000000  1.000000     0.0  0.030206  0.004061  0.774263   \n",
      "725253      0.000000  0.000000     1.0  0.000000  0.000000  0.000000   \n",
      "727088      0.000000  0.030206     0.0  1.000000  0.002981  0.000000   \n",
      "724768      0.000000  0.004061     0.0  0.002981  1.000000  0.000000   \n",
      "...              ...       ...     ...       ...       ...       ...   \n",
      "785459      0.240759  0.008408     0.0  0.006170  0.015087  0.000000   \n",
      "823647      0.000000  0.000000     0.0  0.000000  0.000000  0.000000   \n",
      "732747      0.000000  0.008316     0.0  0.006103  0.004687  0.000000   \n",
      "731861      0.000000  0.004249     0.0  0.003118  0.002395  0.000000   \n",
      "773275      0.055109  0.000000     0.0  0.000000  0.000000  0.000000   \n",
      "\n",
      "product_id    727638    724295    727310    724794  ...    730759    728094  \\\n",
      "product_id                                          ...                       \n",
      "723227      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "725786      0.003829  0.000000  0.027920  0.045046  ...  0.007339  0.445600   \n",
      "725253      0.049576  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "727088      0.002810  0.017667  0.975890  0.005939  ...  0.000000  0.003070   \n",
      "724768      0.002158  0.000000  0.002755  0.004561  ...  0.003956  0.002358   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "785459      0.004468  0.000000  0.005703  0.009442  ...  0.008191  0.004881   \n",
      "823647      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "732747      0.004419  0.000000  0.005641  0.009339  ...  0.000000  0.004828   \n",
      "731861      0.002258  0.000000  0.002882  0.004772  ...  0.000000  0.002467   \n",
      "773275      0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "\n",
      "product_id    817288  776760    780245    785459  823647    732747    731861  \\\n",
      "product_id                                                                     \n",
      "723227      0.000000     0.0  0.000000  0.240759     0.0  0.000000  0.000000   \n",
      "725786      0.004696     0.0  0.004270  0.008408     0.0  0.008316  0.004249   \n",
      "725253      0.000000     0.0  0.000000  0.000000     0.0  0.000000  0.000000   \n",
      "727088      0.036044     0.0  0.003134  0.006170     0.0  0.006103  0.003118   \n",
      "724768      0.002647     0.0  0.002407  0.015087     0.0  0.004687  0.002395   \n",
      "...              ...     ...       ...       ...     ...       ...       ...   \n",
      "785459      0.005479     0.0  0.004983  1.000000     0.0  0.009703  0.004958   \n",
      "823647      0.000000     0.0  0.000000  0.000000     1.0  0.000000  0.000000   \n",
      "732747      0.005419     0.0  0.004928  0.009703     0.0  1.000000  0.004903   \n",
      "731861      0.084101     0.0  0.002518  0.004958     0.0  0.004903  1.000000   \n",
      "773275      0.000000     0.0  0.000000  0.061850     0.0  0.000000  0.000000   \n",
      "\n",
      "product_id    773275  \n",
      "product_id            \n",
      "723227      0.055109  \n",
      "725786      0.000000  \n",
      "725253      0.000000  \n",
      "727088      0.000000  \n",
      "724768      0.000000  \n",
      "...              ...  \n",
      "785459      0.061850  \n",
      "823647      0.000000  \n",
      "732747      0.000000  \n",
      "731861      0.000000  \n",
      "773275      1.000000  \n",
      "\n",
      "[5708 rows x 5708 columns]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(merged_df['combined_features'])\n",
    "print(tfidf_matrix.shape)\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the words\n",
    "print(words[9000])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=words)\n",
    "#tfidf_df.to_csv('tfidf_matrix.csv', index=False)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Convert the cosine similarity matrix to a DataFrame for easy interpretation\n",
    "similarity_df = pd.DataFrame(cosine_sim, index=merged_df['product_id'], columns=merged_df['product_id'])\n",
    "\n",
    "# Sort each column individually in descending order\n",
    "\n",
    "\n",
    "print(similarity_df)\n",
    "print('723227' in similarity_df.index)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:59.411082Z",
     "start_time": "2024-08-09T21:57:58.004086Z"
    }
   },
   "id": "d84c7c499575474a",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 2 most similar items to item 726479\n",
      "product_id\n",
      "768343    1.0\n",
      "827011    1.0\n",
      "827018    1.0\n",
      "725356    1.0\n",
      "744605    1.0\n",
      "Name: 726479, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_product_id = 726479\n",
    "\n",
    "# Extract similarity scores for the target product_id\n",
    "similarities = similarity_df[target_product_id]\n",
    "\n",
    "# Drop the target product_id from the results (since it's the most similar to itself)\n",
    "#similarities = similarities.drop(target_product_id)\n",
    "\n",
    "# Sort the similarity scores in descending order and get the top 2 items\n",
    "top_similar_items = similarities.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Print the top 2 most similar items\n",
    "print(\"Top 2 most similar items to item\", target_product_id)\n",
    "print(top_similar_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:59.422083Z",
     "start_time": "2024-08-09T21:57:59.412082Z"
    }
   },
   "id": "14f9ebd798aae11d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          \n",
      "1                          \n",
      "2                          \n",
      "3        قیمت مناسب زیپ دار\n",
      "4         قیمت عالی و کیفیت\n",
      "               ...         \n",
      "5703               ماندگاری\n",
      "5704                   زیبا\n",
      "5705                       \n",
      "5706                   عالی\n",
      "5707                       \n",
      "Name: advantages, Length: 5708, dtype: object\n"
     ]
    }
   ],
   "source": [
    "###cleaning comment data\n",
    "import re\n",
    "# Replace 'nan' text with empty string\n",
    "merged_df['advantages'] = merged_df['advantages'].str.replace('nan', '', regex=False)  # Remove 'nan'\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].str.replace('nan', '', regex=False)\n",
    "\n",
    "# Remove '\\r' characters\n",
    "merged_df['advantages'] = merged_df['advantages'].str.replace('r', '', regex=False)\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].str.replace('r', '', regex=False)\n",
    "\n",
    "merged_df['advantages'] = merged_df['advantages'].str.replace('\\\\', '', regex=False)\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].str.replace('\\\\', '', regex=False)\n",
    "\n",
    "merged_df['advantages'] = merged_df['advantages'].fillna('')\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].fillna('')\n",
    "# Display the cleaned DataFrame\n",
    "merged_df['advantages'] = merged_df['advantages'].apply(lambda x: re.sub(r'[^\\w\\s,]', '', x))  # Remove all punctuation except ','\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].apply(lambda x: re.sub(r'[^\\w\\s,]', '', x))  # Remove all punctuation except ','\n",
    "\n",
    "merged_df['advantages'] = merged_df['advantages'].str.replace(',', ' ', regex=False)\n",
    "merged_df['disadvantages'] = merged_df['disadvantages'].str.replace(',', ' ', regex=False)\n",
    "\n",
    "print(merged_df['advantages'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:59.487083Z",
     "start_time": "2024-08-09T21:57:59.423084Z"
    }
   },
   "id": "ac41f66ac731726a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       این اسپری دارای فرمول انحصاری برای کشور انگلست...\n",
      "1       با سلام چند وقتی خریداری کردم موتور داغون هنوز...\n",
      "2                                     خوبه ارزش خرید داره\n",
      "3       قیمت مناسب زیپ دار   دو تیکه پارچه به شکل کامل...\n",
      "4             قیمت عالی و کیفیت  جنس و کیفیت وقیمتش عالیه\n",
      "                              ...                        \n",
      "5703    ماندگاری بوی معمولی این اسپری بوی همه چیز میده...\n",
      "5704            زیبا ندارد خیلی خیلی آب رو گرم نگه میدارد\n",
      "5705               خیلی چیز خوبیه راضیم خیلی به درد بخوره\n",
      "5706    عالی   فوق العاده بی کیفیت سلام ممنون ازبابت ب...\n",
      "5707    عایه من خریدم خیلی راضی هستم ازش سلام دوستان ک...\n",
      "Name: combined_text, Length: 5708, dtype: object\n"
     ]
    }
   ],
   "source": [
    "merged_df['combined_text'] = merged_df['advantages'] + ' ' + merged_df['disadvantages'] + ' ' + merged_df['comment']\n",
    "\n",
    "# Strip any leading/trailing whitespace\n",
    "merged_df['combined_text'] = merged_df['combined_text'].str.strip()\n",
    "print(merged_df['combined_text'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:57:59.507084Z",
     "start_time": "2024-08-09T21:57:59.488083Z"
    }
   },
   "id": "69d52a6281095336",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      product_id                                     processed_text\n",
      "0         723227  اسپری فرمول انحصاری کشور انگلستان هست فوق‌العا...\n",
      "1         725786  سلام خریداری موتور داغون یکهفته نشده خواب میمو...\n",
      "2         725253                                خوبه ارزش خرید داره\n",
      "3         727088  قیمت زیپ تیکه پارچه شکل ساده بهم دوخته سرش یه ...\n",
      "4         724768                  قیمت کیفیت جنس کیفیت وقیمتش عالیه\n",
      "...          ...                                                ...\n",
      "5703      785459  ماندگاری بوی معمولی اسپری بوی میده جزء دیزایر بلو\n",
      "5704      823647                           زیبا آب گرم نگه داشت#دار\n",
      "5705      732747                                خوبیه راض درد بخوره\n",
      "5706      731861  فوق‌العاده بی‌کیفیت سلام ممنون ازباب موقع سفار...\n",
      "5707      773275  عایه خرید#خر راضی #هست ازش سلام دوستان کیفیت ،...\n",
      "\n",
      "[5708 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from hazm import *\n",
    "\n",
    "# Initialize Hazm components\n",
    "normalizer = Normalizer()\n",
    "word_tokenizer = WordTokenizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "stopwords = stopwords_list()\n",
    "\n",
    "# Step 1: Normalize the text in the 'combined_text' column\n",
    "merged_df['normalized_text'] = merged_df['combined_text'].apply(lambda x: normalizer.normalize(x))\n",
    "\n",
    "# Step 2: Tokenize the normalized text into words\n",
    "merged_df['tokenized_text'] = merged_df['normalized_text'].apply(lambda x: word_tokenizer.tokenize(x))\n",
    "\n",
    "# Step 3: Remove stopwords from the tokenized text\n",
    "merged_df['filtered_text'] = merged_df['tokenized_text'].apply(lambda words: [word for word in words if word not in stopwords])\n",
    "\n",
    "# Step 4: Lemmatize the filtered words\n",
    "merged_df['lemmatized_text'] = merged_df['filtered_text'].apply(lambda words: [lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "# Step 5: Combine the lemmatized words back into a single string\n",
    "merged_df['processed_text'] = merged_df['lemmatized_text'].apply(lambda words: ' '.join(words))\n",
    "\n",
    "# Display the DataFrame with the new processed text\n",
    "print(merged_df[['product_id', 'processed_text']])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:58:07.762085Z",
     "start_time": "2024-08-09T21:57:59.510085Z"
    }
   },
   "id": "232cb53d64a5c32e",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape for combined_features: (5708, 14315)\n",
      "TF-IDF matrix shape for comments: (5708, 17504)\n",
      "product_id    723227    725786    725253    727088    724768    724514  \\\n",
      "product_id                                                               \n",
      "723227      1.000000  0.000000  0.000000  0.011409  0.000000  0.000000   \n",
      "725786      0.000000  1.000000  0.000000  0.028762  0.002437  0.464558   \n",
      "725253      0.000000  0.000000  1.000000  0.056827  0.000000  0.019641   \n",
      "727088      0.011409  0.028762  0.056827  1.000000  0.016512  0.002790   \n",
      "724768      0.000000  0.002437  0.000000  0.016512  1.000000  0.000000   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "785459      0.183208  0.005045  0.000000  0.003702  0.009052  0.000000   \n",
      "823647      0.004057  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "732747      0.000000  0.004989  0.000000  0.022956  0.002812  0.000000   \n",
      "731861      0.019250  0.010817  0.000000  0.013338  0.038923  0.000000   \n",
      "773275      0.033066  0.009145  0.000000  0.015824  0.029558  0.000000   \n",
      "\n",
      "product_id    727638    724295    727310    724794  ...    730759    728094  \\\n",
      "product_id                                          ...                       \n",
      "723227      0.007741  0.002211  0.001401  0.015463  ...  0.003182  0.013977   \n",
      "725786      0.006471  0.002490  0.018837  0.027027  ...  0.008776  0.267360   \n",
      "725253      0.053202  0.000000  0.027636  0.000000  ...  0.000000  0.000000   \n",
      "727088      0.016108  0.013847  0.617211  0.003563  ...  0.003822  0.011996   \n",
      "724768      0.014319  0.000000  0.001653  0.002737  ...  0.002374  0.060835   \n",
      "...              ...       ...       ...       ...  ...       ...       ...   \n",
      "785459      0.005638  0.000000  0.003422  0.005665  ...  0.004914  0.007333   \n",
      "823647      0.017516  0.000000  0.000000  0.000000  ...  0.009646  0.009141   \n",
      "732747      0.019234  0.003927  0.009961  0.005603  ...  0.006897  0.009432   \n",
      "731861      0.003914  0.001838  0.004808  0.002863  ...  0.003229  0.013667   \n",
      "773275      0.002831  0.024989  0.003405  0.000000  ...  0.003571  0.060218   \n",
      "\n",
      "product_id    817288    776760    780245    785459    823647    732747  \\\n",
      "product_id                                                               \n",
      "723227      0.000000  0.035760  0.000000  0.183208  0.004057  0.000000   \n",
      "725786      0.002818  0.012196  0.002562  0.005045  0.000000  0.004989   \n",
      "725253      0.024391  0.000000  0.048873  0.000000  0.000000  0.000000   \n",
      "727088      0.027170  0.014836  0.008824  0.003702  0.000000  0.022956   \n",
      "724768      0.001588  0.016706  0.036225  0.009052  0.000000  0.002812   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "785459      0.003288  0.005711  0.002990  1.000000  0.000000  0.043409   \n",
      "823647      0.013484  0.014849  0.000000  0.000000  1.000000  0.036722   \n",
      "732747      0.019181  0.013045  0.002957  0.043409  0.036722  1.000000   \n",
      "731861      0.082316  0.023034  0.032280  0.002975  0.000000  0.028145   \n",
      "773275      0.004124  0.023556  0.000000  0.037110  0.009507  0.016611   \n",
      "\n",
      "product_id    731861    773275  \n",
      "product_id                      \n",
      "723227      0.019250  0.033066  \n",
      "725786      0.010817  0.009145  \n",
      "725253      0.000000  0.000000  \n",
      "727088      0.013338  0.015824  \n",
      "724768      0.038923  0.029558  \n",
      "...              ...       ...  \n",
      "785459      0.002975  0.037110  \n",
      "823647      0.000000  0.009507  \n",
      "732747      0.028145  0.016611  \n",
      "731861      1.000000  0.015441  \n",
      "773275      0.015441  1.000000  \n",
      "\n",
      "[5708 rows x 5708 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer_features = TfidfVectorizer()\n",
    "tfidf_matrix_features = tfidf_vectorizer_features.fit_transform(merged_df['combined_features'])\n",
    "print(f\"TF-IDF matrix shape for combined_features: {tfidf_matrix_features.shape}\")\n",
    "\n",
    "# Vectorize comments\n",
    "tfidf_vectorizer_comments = TfidfVectorizer()\n",
    "tfidf_matrix_comments = tfidf_vectorizer_comments.fit_transform(merged_df['combined_text'])\n",
    "print(f\"TF-IDF matrix shape for comments: {tfidf_matrix_comments.shape}\")\n",
    "\n",
    "# Calculate cosine similarity for combined_features\n",
    "cosine_sim_features = cosine_similarity(tfidf_matrix_features, tfidf_matrix_features)\n",
    "\n",
    "# Calculate cosine similarity for comments\n",
    "cosine_sim_comments = cosine_similarity(tfidf_matrix_comments, tfidf_matrix_comments)\n",
    "\n",
    "# Weighting\n",
    "title_category_weight = 0.6\n",
    "comments_weight = 0.4\n",
    "\n",
    "# Normalize the weights (assuming we want the weights to add up to 1)\n",
    "combined_similarity = (title_category_weight * cosine_sim_features +\n",
    "                       comments_weight * cosine_sim_comments)\n",
    "\n",
    "# Create a DataFrame from the combined similarity matrix\n",
    "similarity_df2 = pd.DataFrame(combined_similarity, index=merged_df['product_id'], columns=merged_df['product_id'])\n",
    "\n",
    "# Display the combined similarity matrix\n",
    "print(similarity_df2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:58:09.841088Z",
     "start_time": "2024-08-09T21:58:07.763083Z"
    }
   },
   "id": "f918b5b74daf8fc9",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar items to item 726479\n",
      "product_id\n",
      "726479    1.000000\n",
      "744555    0.714731\n",
      "725421    0.699731\n",
      "827018    0.681821\n",
      "768343    0.661208\n",
      "Name: 726479, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target_product_id = 726479\n",
    "\n",
    "# Extract similarity scores for the target product_id\n",
    "similarities = similarity_df2[target_product_id]\n",
    "\n",
    "# Drop the target product_id from the results (since it's the most similar to itself)\n",
    "#similarities = similarities.drop(target_product_id)\n",
    "\n",
    "# Sort the similarity scores in descending order and get the top 2 items\n",
    "top_similar_items = similarities.sort_values(ascending=False).head(5)\n",
    "\n",
    "# Print the top 2 most similar items\n",
    "print(\"Top 5 most similar items to item\", target_product_id)\n",
    "print(top_similar_items)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:58:09.854086Z",
     "start_time": "2024-08-09T21:58:09.844086Z"
    }
   },
   "id": "cc47f9171c361f3d",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:58:09.859087Z",
     "start_time": "2024-08-09T21:58:09.856085Z"
    }
   },
   "id": "8fd161409dc57602",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T21:58:09.865086Z",
     "start_time": "2024-08-09T21:58:09.861085Z"
    }
   },
   "id": "a8efe61b3152bf1",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
